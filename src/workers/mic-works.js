import * as ort from 'onnxruntime-web/webgpu';

// åˆ©ç”¨ IndexedDB å®ç°æ¨¡å‹æ–‡ä»¶ç¼“å­˜
const DB_NAME = "mic-onnx-models";
const STORE_NAME = "models";
const DB_VERSION = 1;

// æ‰“å¼€æˆ–å‡çº§ IndexedDB
function openModelDB() {
    return new Promise((resolve, reject) => {
        const req = indexedDB.open(DB_NAME, DB_VERSION);
        req.onupgradeneeded = function (event) {
            const db = req.result;
            if (!db.objectStoreNames.contains(STORE_NAME)) {
                db.createObjectStore(STORE_NAME, { keyPath: "modelUrl" });
            }
        };
        req.onsuccess = function () {
            resolve(req.result);
        };
        req.onerror = function (e) {
            reject(e);
        };
    });
}

// ä» IndexedDB è·å–ç¼“å­˜çš„æ¨¡å‹
async function getCachedModel(modelUrl) {
    const db = await openModelDB();
    return new Promise((resolve, reject) => {
        const tx = db.transaction([STORE_NAME], "readonly");
        const store = tx.objectStore(STORE_NAME);
        const req = store.get(modelUrl);
        req.onsuccess = function () {
            resolve(req.result ? req.result.data : null);
        };
        req.onerror = function (e) {
            reject(e);
        };
    });
}

// å°†æ¨¡å‹ç¼“å­˜åœ¨ IndexedDB
async function cacheModel(modelUrl, arrayBuffer) {
    const db = await openModelDB();
    return new Promise((resolve, reject) => {
        const tx = db.transaction([STORE_NAME], "readwrite");
        const store = tx.objectStore(STORE_NAME);
        store.put({ modelUrl, data: arrayBuffer });
        tx.oncomplete = function () {
            resolve();
        };
        tx.onerror = function (e) {
            reject(e);
        };
    });
}

function validateOnnxBuffer(buffer) {
    const size = buffer.byteLength;

    // 1ï¸âƒ£ å¤§å°ä¸‹é™ï¼ˆæ ¹æ®ä½ æ¨¡å‹è°ƒï¼Œ5â€“10MB å¾ˆå¸¸è§ï¼‰
    if (size < 1024 * 1024) {
        throw new Error(`ONNX file too small (${size} bytes)`);
    }

    // 2ï¸âƒ£ é˜² HTML / JSON / æ–‡æœ¬
    const head = new Uint8Array(buffer.slice(0, 16));
    const isText = head.every(b => b >= 9 && b <= 126);
    if (isText) {
        const text = new TextDecoder().decode(head);
        throw new Error(`Downloaded file looks like text: ${text}`);
    }
}

async function downloadModelArrayBuffer(modelUrl, progressCb) {
    const response = await fetch(modelUrl, {
        cache: "no-store",
        headers: {
            "Accept": "application/octet-stream",
        },
    });

    if (!response.ok) {
        throw new Error(`Failed to download model ${modelUrl}: ${response.status}`);
    }

    const contentLength = response.headers.get("content-length");
    const total = contentLength ? parseInt(contentLength, 10) : null;

    if (!response.body) {
        throw new Error("Response body is null");
    }

    const reader = response.body.getReader();
    const chunks = [];
    let receivedLength = 0;

    progressCb && self.postMessage({ status: "download_start", modelUrl, total });

    while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        chunks.push(value);
        receivedLength += value.byteLength;

        progressCb && self.postMessage({
            status: "download_progress",
            modelUrl,
            received: receivedLength,
            total,
            progress: total ? receivedLength / total * 100 : -1,
        });
    }

    const buffer = new Uint8Array(receivedLength);
    let offset = 0;
    for (const chunk of chunks) {
        buffer.set(chunk, offset);
        offset += chunk.byteLength;
    }

    // ğŸ”¥ å…³é”®ï¼šæ ¡éªŒ
    validateOnnxBuffer(buffer.buffer);

    progressCb && self.postMessage({
        status: "download_complete",
        modelUrl,
        size: receivedLength,
    });

    return buffer.buffer;
}


class MicPredictPipeline {
    static modelSessions = {};
    static sessionPromises = {};
    static creatingSession = false; // å…¨å±€é”ï¼Œç¡®ä¿åŒä¸€æ—¶é—´åªæœ‰ä¸€ä¸ªä¼šè¯åœ¨åˆ›å»º

    static async getSession(modelUrl) {
        // å¦‚æœä¼šè¯å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
        if (this.modelSessions[modelUrl]) {
            return this.modelSessions[modelUrl];
        }
        // å¦‚æœæ­£åœ¨åˆ›å»ºè¯¥ä¼šè¯ï¼Œç­‰å¾…åˆ›å»ºå®Œæˆ
        if (this.sessionPromises[modelUrl]) {
            return this.sessionPromises[modelUrl];
        }

        // ç­‰å¾…å…¶ä»–ä¼šè¯åˆ›å»ºå®Œæˆ
        while (this.creatingSession) {
            await new Promise(resolve => setTimeout(resolve, 10));
        }

        // å†æ¬¡æ£€æŸ¥ï¼Œå¯èƒ½åœ¨ç­‰å¾…æœŸé—´å·²ç»åˆ›å»ºå®Œæˆ
        if (this.modelSessions[modelUrl]) {
            return this.modelSessions[modelUrl];
        }
        if (this.sessionPromises[modelUrl]) {
            return this.sessionPromises[modelUrl];
        }

        // è®¾ç½®åˆ›å»ºé”
        this.creatingSession = true;
        const sessionPromise = (async () => {
            // å°è¯•è¯»å–ç¼“å­˜
            console.log("å°è¯•è¯»å–ç¼“å­˜", modelUrl);
            let modelBuffer = await getCachedModel(modelUrl);
            if (!modelBuffer) {
                // ä¸‹è½½æ¨¡å‹å¹¶ç¼“å­˜ï¼ˆä¼ é€’è¿›åº¦å›è°ƒï¼‰
                console.log("ä¸‹è½½æ¨¡å‹å¹¶ç¼“å­˜", modelUrl);
                modelBuffer = await downloadModelArrayBuffer(modelUrl, true);
                console.log("ä¸‹è½½æ¨¡å‹å¹¶ç¼“å­˜å®Œæˆ", modelUrl);
                await cacheModel(modelUrl, modelBuffer);
                console.log("ç¼“å­˜æ¨¡å‹", modelUrl);
            }
            const sess = await ort.InferenceSession.create(
                modelBuffer,
                {
                    executionProviders: ["webgpu"],
                    logSeverityLevel: 3,
                    logVerbosityLevel: 0,
                }
            );
            this.modelSessions[modelUrl] = { session: sess, ort };
            this.creatingSession = false; // é‡Šæ”¾é”
            return this.modelSessions[modelUrl];
        })().catch(err => {
            delete this.sessionPromises[modelUrl];
            this.creatingSession = false; // é‡Šæ”¾é”
            throw err;
        });
        this.sessionPromises[modelUrl] = sessionPromise;
        return sessionPromise;
    }

    static async predict({ embedding, modelUrl, target }) {
        return await this.predictEmbedding({ embedding, modelUrl });
    }

    static async predictEmbedding({ embedding, modelUrl, device = "webgpu" }) {
        const { session, ort } = await this.getSession(modelUrl);
        let inputArray;
        if (embedding instanceof Float32Array) {
            inputArray = embedding;
        } else if (Array.isArray(embedding)) {
            inputArray = new Float32Array(embedding);
        } else {
            throw new Error("Invalid embedding format");
        }
        let dim0 = 1;
        let dim2 = 320;
        if (inputArray.length % (dim0 * dim2) !== 0) {
            throw new Error(`è¾“å…¥é•¿åº¦ (${inputArray.length}) ä¸èƒ½æ•´é™¤ 320`);
        }
        let dim1 = inputArray.length / (dim0 * dim2);
        if (!Number.isInteger(dim1)) {
            throw new Error(`ä¸­é—´ç»´åº¦ä¸æ˜¯æ•´æ•°ï¼Œæ— æ³•æ¨æ–­å½¢çŠ¶: ${inputArray.length} / (${dim0}*${dim2})`);
        }
        const inputTensor = new ort.Tensor("float32", inputArray, [dim0, dim1, dim2]);
        const inputName = session.inputNames[0];
        const feeds = { [inputName]: inputTensor };

        const outputMap = await session.run(feeds);
        const outputName = session.outputNames[0];
        const resultTensor = outputMap[outputName];

        const value = Array.isArray(resultTensor.data)
            ? resultTensor.data[0]
            : resultTensor.data;

        return {
            mic: value,
            outputRaw: resultTensor.data
        };
    }
}

self.addEventListener("message", async (event) => {
    try {
        const { embedding, modelUrl, device } = event.data || {};
        self.postMessage({
            status: "computing",
            message: "æ­£åœ¨è®¡ç®—MIC..."
        });
        const result = await MicPredictPipeline.predictEmbedding({ embedding, modelUrl, device });
        self.postMessage({
            status: "complete",
            mic: result.mic,
            outputRaw: result.outputRaw
        });
    } catch (error) {
        self.postMessage({
            status: "error",
            error: error.message || String(error)
        });
    }
});
